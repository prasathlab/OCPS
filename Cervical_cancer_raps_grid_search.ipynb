{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importing necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-21T10:49:15.274934Z","iopub.status.busy":"2024-08-21T10:49:15.274499Z","iopub.status.idle":"2024-08-21T10:49:19.477604Z","shell.execute_reply":"2024-08-21T10:49:19.476034Z","shell.execute_reply.started":"2024-08-21T10:49:15.274899Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["# CP method metrices(specific to Cervical Cancer Dataset):- "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-21T10:49:22.444369Z","iopub.status.busy":"2024-08-21T10:49:22.443370Z","iopub.status.idle":"2024-08-21T10:49:22.458163Z","shell.execute_reply":"2024-08-21T10:49:22.456813Z","shell.execute_reply.started":"2024-08-21T10:49:22.444329Z"},"trusted":true},"outputs":[],"source":["def avg_set_size_metric(conformal_set):\n","    lengths = torch.sum(conformal_set, dim=1)\n","    avg_set_size_len = torch.sum(lengths)/conformal_set.shape[0]\n","    return avg_set_size_len\n","\n","\n","\n","\n","def coverage_gap_metric(conformal_set, df_true_class_test, alpha):\n","    true_class = conformal_set[range(conformal_set.shape[0]), df_true_class_test]\n","    tensor_sum = torch.sum(true_class)\n","    coverage = tensor_sum/true_class.shape[0]\n","    coverage_gap = (abs((1-alpha) - coverage)/(1-alpha))*100\n","    return coverage_gap, coverage\n","\n","\n","\n","\n","\n","def class_Overlap_metric(conformal_set, label):\n","\n","    overlap_count = 0\n","\n","    for i in range(conformal_set.shape[0]):\n","        current_set = conformal_set[i]\n","\n","        if (label[i] == 0 or label[i] == 1 or label[i] == 2) and (current_set[4] == 1 or current_set[6] == 1 or current_set[7] == 1 or current_set[3] == 1 or current_set[5] == 1):\n","                overlap_count += 1\n","            \n","        elif (label[i] == 4 or label[i] == 6 or  label[i] == 7) and (current_set[0] == 1 or current_set[1] == 1 or current_set[2] or current_set[3] == 1 or current_set[5] == 1):\n","                overlap_count += 1\n","\n","        elif (label[i] == 3 or label[i] == 5) and (current_set[4] == 1 or current_set[6] == 1 or current_set[7] == 1 or current_set[0] == 1 or current_set[1] == 1 or current_set[2] == 1):\n","                overlap_count += 1\n","\n","\n","    perecentage_of_overlap  = (overlap_count/conformal_set.shape[0])*100\n","\n","    return perecentage_of_overlap\n","\n","\n","\n","\n","\n","\n","def confusion_set_Overlap_metric(conformal_set, label):\n","\n","    overlap_count = 0\n","\n","    for i in range(conformal_set.shape[0]):\n","        current_set = conformal_set[i]\n","\n","        if (label[i] == 4) and (current_set[6] == 1):\n","            overlap_count += 1\n","\n","        elif (label[i] == 6) and (current_set[4] == 1):\n","            overlap_count += 1\n","\n","        \n","        elif (label[i] == 3) and (current_set[5] == 1):\n","            overlap_count += 1\n","\n","        elif (label[i] == 5) and (current_set[3] == 1):\n","            overlap_count += 1\n","\n","    perecentage_of_overlap  = (overlap_count/conformal_set.shape[0])*100\n","\n","    return perecentage_of_overlap"]},{"cell_type":"markdown","metadata":{},"source":["# RAPS class :- "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-21T10:49:24.568429Z","iopub.status.busy":"2024-08-21T10:49:24.567501Z","iopub.status.idle":"2024-08-21T10:49:24.583290Z","shell.execute_reply":"2024-08-21T10:49:24.581875Z","shell.execute_reply.started":"2024-08-21T10:49:24.568387Z"},"trusted":true},"outputs":[],"source":["class RAPS():\n","    def __init__(self, softmax, true_class, alpha, k_reg, lambd, rand=True):\n","        self.prob_output = softmax\n","        self.true_class = true_class\n","        self.alpha = (1 - alpha) * (1 + (1 / softmax.shape[0]))\n","        self.k_reg = k_reg\n","        self.lambd = lambd\n","        self.rand = rand\n","        \n","        \n","        \n","    def conformal_score(self):\n","        conformal_score = []\n","        for i in range(self.prob_output.shape[0]):\n","            true_class_prob = self.prob_output[i][self.true_class[i]]\n","            current_class_prob = self.prob_output[i]\n","            sorted_class_prob, _ = torch.sort(current_class_prob, descending=True)\n","            index = torch.nonzero(sorted_class_prob == true_class_prob).item()\n","            cumulative_sum = torch.sum(sorted_class_prob[:index + 1])\n","            \n","            if index - self.k_reg > 0:\n","               cumulative_sum = cumulative_sum + self.lambd*(index - self.k_reg)\n","            \n","            if self.rand:\n","                U = torch.rand(1).item()\n","                cumulative_sum = cumulative_sum - U*sorted_class_prob[index]\n","\n","            conformal_score.append(cumulative_sum)\n","        \n","        conformal_score = torch.tensor(conformal_score)\n","        \n","        return conformal_score\n","    \n","    \n","    \n","    def quantile(self):\n","        conformal_scores = self.conformal_score()\n","        quantile_value = torch.quantile(conformal_scores, self.alpha)\n","        return quantile_value\n","    \n","    \n","    \n","    def prediction(self, softmax, quantile_value):\n","        prob_output = softmax\n","        prediction = torch.zeros(prob_output.shape[0], prob_output.shape[1])\n","        for i in range(prob_output.shape[0]):\n","            current_class_prob = prob_output[i]\n","            sorted_class_prob, _ = torch.sort(current_class_prob, descending=True)\n","            sum = 0\n","            j = 0\n","            for idx in range(len(sorted_class_prob)):\n","                if sum <= quantile_value:\n","                    sum += sorted_class_prob[idx]\n","                    if idx - self.k_reg > 0:\n","                        sum = sum + self.lambd*(idx - self.k_reg)\n","                    j += 1\n","                else:\n","                    break\n","                    \n","            \"\"\"\n","            \n","            if self.rand:\n","                U = torch.rand(1).item()\n","                if j != prob_output.shape[1]:\n","                    N = torch.sum(sorted_class_prob[:j + 1]) - quantile_value\n","                else:\n","                    N = torch.sum(sorted_class_prob[:j]) - quantile_value\n","                if idx - self.k_reg > 0:\n","                    N += self.lambd*(j - self.k_reg)\n","\n","                if j != prob_output.shape[1]:\n","                    D = sorted_class_prob[j]\n","                else:\n","                    D = sorted_class_prob[j-1]\n","                if idx - self.k_reg > 0:\n","                    D += self.lambd\n","\n","                if N/D <= U:\n","                    j = j -1\n","                    \n","                \"\"\" \n","        \n","            \n","            \n","            \n","                    \n","            for idx in range(j):\n","                index = torch.nonzero(current_class_prob == sorted_class_prob[idx]).item()\n","                prediction[i][index] = 1.0\n","                \n","        return prediction"]},{"cell_type":"markdown","metadata":{},"source":["# Main function :- "]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-08-21T12:02:19.271707Z","iopub.status.busy":"2024-08-21T12:02:19.271286Z","iopub.status.idle":"2024-08-21T12:02:19.311513Z","shell.execute_reply":"2024-08-21T12:02:19.310195Z","shell.execute_reply.started":"2024-08-21T12:02:19.271675Z"},"trusted":true},"outputs":[],"source":["\n","def main(expt_no, Trials, alpha,k_reg, lambd, rand):\n","    avg_set_size_len_for_T_trials = []\n","    avg_coverage_gap_for_T_trials = []\n","    avg_coverage_for_T_trials = []\n","    \n","    normal_avg_set_size_len_for_T_trials = []\n","    normal_avg_coverage_gap_for_T_trials = []\n","    normal_avg_coverage_for_T_trials = []\n","\n","    LG_avg_set_size_len_for_T_trials = []\n","    LG_avg_coverage_gap_for_T_trials = []\n","    LG_avg_coverage_for_T_trials = []\n","\n","    HG_avg_set_size_len_for_T_trials = []\n","    HG_avg_coverage_gap_for_T_trials = []\n","    HG_avg_coverage_for_T_trials = []\n","    \n","    perecentage_of_overlap_for_T_trials = []\n","\n","    confusion_set_Overlap_metric_for_T_trials = []\n","    \n","    \n","        \n","    for t in range(Trials):\n","        \"\"\"\n","        \n","        print()\n","        print(f'Trials :- {t}')\n","        print()\n","        \"\"\"\n","        \n","        \n","        # loading the annotation file :-\n","        #Expt1:-\n","        #path = ''\n","        #df = pd.read_csv(path)\n","        \n","        #Expt2:-\n","        #path = ''\n","        #df = pd.read_csv(path)\n","        \n","        #Expt3:-\n","        #path = ''\n","        #df = pd.read_csv(path)\n","        \n","        #Expt4:-\n","        #path = ''\n","        #df = pd.read_csv(path)\n","        \n","        \n","        df = df.sample(frac=1).reset_index(drop=True)\n","        \n","        # calib-test split :- \n","        test_0, calib_0, test_1, calib_1, test_2, calib_2, test_3, calib_3, test_4, calib_4, test_5, calib_5, test_6, calib_6, test_7, calib_7, test_label, calib_label = train_test_split(df['0'],df['1'], df['2'],df['3'], df['4'],df['5'], df['6'],df['7'], df['Label'], test_size = 0.1, stratify=df['Label'], random_state=42)\n","\n","        test_0 = test_0.to_frame()\n","        test_1 = test_1.to_frame()\n","        test_2 = test_2.to_frame()\n","        test_3 = test_3.to_frame()\n","        test_4 = test_4.to_frame()\n","        test_5 = test_5.to_frame()\n","        test_6 = test_6.to_frame()\n","        test_7 = test_7.to_frame()\n","        test = test_0.join(test_1, how='inner').join(test_2, how='inner').join(test_3, how='inner').join(test_4, how='inner').join(test_5, how='inner').join(test_6, how='inner').join(test_7, how='inner').join(test_label, how='inner')\n","        test = test.reset_index(drop=True)\n","\n","        calib_0 = calib_0.to_frame()\n","        calib_1 = calib_1.to_frame()\n","        calib_2 = calib_2.to_frame()\n","        calib_3 = calib_3.to_frame()\n","        calib_4 = calib_4.to_frame()\n","        calib_5 = calib_5.to_frame()\n","        calib_6 = calib_6.to_frame()\n","        calib_7 = calib_7.to_frame()\n","        calib = calib_0.join(calib_1, how='inner').join(calib_2, how='inner').join(calib_3, how='inner').join(calib_4, how='inner').join(calib_5, how='inner').join(calib_6, how='inner').join(calib_7, how='inner').join(calib_label, how='inner')\n","        calib = calib.reset_index(drop=True)\n","\n","        prob_output = calib.iloc[:,:-1]\n","        df_np = prob_output.values \n","        df_prob_output_calib = torch.tensor(df_np, dtype=torch.float32)\n","\n","        prob_output = test.iloc[:,:-1]\n","        df_np = prob_output.values\n","        df_prob_output_test = torch.tensor(df_np, dtype=torch.float32)\n","        \n","        true_class = calib.iloc[:,-1]\n","        df_np = true_class.values\n","        df_true_class_calib = torch.tensor(df_np, dtype=torch.int)\n","\n","\n","        true_class = test.iloc[:,-1]\n","        df_np = true_class.values\n","        df_true_class_test = torch.tensor(df_np, dtype=torch.int)\n","        \n","        \n","        \n","        conformal_wrapper = RAPS(df_prob_output_calib, df_true_class_calib, alpha, k_reg, lambd, rand)\n","        quantile_value = conformal_wrapper.quantile()\n","        #print(f'quantile_value :- {quantile_value}')\n","        \n","        conformal_set = conformal_wrapper.prediction(df_prob_output_test, quantile_value)\n","        \n","        \n","        if expt_no == 1:\n","            avg_set_size = avg_set_size_metric(conformal_set)\n","            coverage_gap, coverage = coverage_gap_metric(conformal_set, df_true_class_test, alpha)    \n","            avg_set_size_len_for_T_trials.append(avg_set_size)\n","            avg_coverage_gap_for_T_trials.append(coverage_gap)\n","            avg_coverage_for_T_trials.append(coverage)\n","            \n","            \n","            \n","        elif expt_no == 2:\n","            label = df_true_class_test\n","            indices_0 = torch.nonzero(label == 0).squeeze()\n","            indices_1 = torch.nonzero(label == 1).squeeze()\n","            indices_2 = torch.nonzero(label == 2).squeeze()\n","            indices_3 = torch.nonzero(label == 3).squeeze()\n","            indices_4 = torch.nonzero(label == 4).squeeze()\n","            indices_5 = torch.nonzero(label == 5).squeeze()\n","            indices_6 = torch.nonzero(label == 6).squeeze()\n","            indices_7 = torch.nonzero(label == 7).squeeze()\n","\n","\n","            Normal_idx = torch.cat((indices_0, indices_1, indices_2))\n","            LG_idx = torch.cat((indices_3, indices_5))\n","            HG_idx = torch.cat((indices_4, indices_6, indices_7))\n","\n","            normal_conformal_prediction_set = conformal_set[Normal_idx, :]\n","            LG_conformal_prediction_set = conformal_set[LG_idx, :]\n","            HG_conformal_prediction_set = conformal_set[HG_idx, :]\n","\n","\n","            normal_avg_set_size_len = avg_set_size_metric(normal_conformal_prediction_set)\n","            LG_avg_set_size_len = avg_set_size_metric(LG_conformal_prediction_set)\n","            HG_avg_set_size_len = avg_set_size_metric(HG_conformal_prediction_set)\n","            \n","            normal_true_class = df_true_class_test[Normal_idx]\n","            LG_true_class = df_true_class_test[LG_idx]\n","            HG_true_class = df_true_class_test[HG_idx]\n","\n","\n","            normal_coverage_gap, normal_coverage = coverage_gap_metric(normal_conformal_prediction_set, normal_true_class, alpha)\n","            LG_coverage_gap, LG_coverage = coverage_gap_metric(LG_conformal_prediction_set, LG_true_class, alpha)\n","            HG_coverage_gap, HG_coverage = coverage_gap_metric(HG_conformal_prediction_set, HG_true_class, alpha)\n","            \n","            \n","            normal_avg_set_size_len_for_T_trials.append(normal_avg_set_size_len)\n","            normal_avg_coverage_gap_for_T_trials.append(normal_coverage_gap)\n","            normal_avg_coverage_for_T_trials.append(normal_coverage)\n","\n","            LG_avg_set_size_len_for_T_trials.append(LG_avg_set_size_len)\n","            LG_avg_coverage_gap_for_T_trials.append(LG_coverage_gap)\n","            LG_avg_coverage_for_T_trials.append(LG_coverage)\n","\n","            HG_avg_set_size_len_for_T_trials.append(HG_avg_set_size_len)\n","            HG_avg_coverage_gap_for_T_trials.append(HG_coverage_gap)\n","            HG_avg_coverage_for_T_trials.append(HG_coverage)\n","            \n","        \n","        elif expt_no == 3:\n","            perecentage_of_overlap = class_Overlap_metric(conformal_set, df_true_class_test)\n","            perecentage_of_overlap_for_T_trials.append(perecentage_of_overlap)\n","            \n","            \n","        elif expt_no == 4:\n","            perecentage_of_confusion = confusion_set_Overlap_metric(conformal_set, df_true_class_test)\n","            confusion_set_Overlap_metric_for_T_trials.append(perecentage_of_confusion)\n","            \n","            \n","            \n","            \n","            \n","            \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","    if expt_no == 1:       \n","        avg_set_size_len_for_T_trials = np.array(avg_set_size_len_for_T_trials)\n","        average_len = np.mean(avg_set_size_len_for_T_trials)\n","        std_dev_len = np.std(avg_set_size_len_for_T_trials, ddof=1)\n","\n","\n","\n","        avg_coverage_gap_for_T_trials = np.array(avg_coverage_gap_for_T_trials)\n","        average_coverage_gap = np.mean(avg_coverage_gap_for_T_trials)\n","        std_dev_coverage_gap = np.std(avg_coverage_gap_for_T_trials, ddof=1)\n","\n","\n","\n","        avg_coverage_for_T_trials = np.array(avg_coverage_for_T_trials)\n","        average_coverage = np.mean(avg_coverage_for_T_trials)\n","        std_dev_coverage = np.std(avg_coverage_for_T_trials, ddof=1)\n","        \n","        \n","        return average_len, std_dev_len, average_coverage_gap, std_dev_coverage_gap, average_coverage, std_dev_coverage\n","        \n","        \n","        \n","        \n","        \n","    elif expt_no == 2:\n","        \n","        #set_size:- \n","        normal_avg_set_size_len_for_T_trials = np.array(normal_avg_set_size_len_for_T_trials)\n","        normal_average_set_size_len = np.mean(normal_avg_set_size_len_for_T_trials)\n","        normal_std_dev_set_size_len = np.std(normal_avg_set_size_len_for_T_trials, ddof=1)\n","        \n","        LG_avg_set_size_len_for_T_trials = np.array(LG_avg_set_size_len_for_T_trials)\n","        LG_average_set_size_len = np.mean(LG_avg_set_size_len_for_T_trials)\n","        LG_std_dev_set_size_len = np.std(LG_avg_set_size_len_for_T_trials, ddof=1)\n","        \n","        HG_avg_set_size_len_for_T_trials = np.array(HG_avg_set_size_len_for_T_trials)\n","        HG_average_set_size_len = np.mean(HG_avg_set_size_len_for_T_trials)\n","        HG_std_dev_set_size_len = np.std(HG_avg_set_size_len_for_T_trials, ddof=1)\n","        \n","        \n","        \n","        #coverage_gap:-\n","        normal_avg_coverage_gap_for_T_trials = np.array(normal_avg_coverage_gap_for_T_trials)\n","        normal_avg_coverage_gap = np.mean(normal_avg_coverage_gap_for_T_trials)\n","        normal_std_dev_coverage_gap = np.std(normal_avg_coverage_gap_for_T_trials)\n","        \n","        LG_avg_coverage_gap_for_T_trials = np.array(LG_avg_coverage_gap_for_T_trials)\n","        LG_avg_coverage_gap = np.mean(LG_avg_coverage_gap_for_T_trials)\n","        LG_std_dev_coverage_gap = np.std(LG_avg_coverage_gap_for_T_trials)\n","        \n","        HG_avg_coverage_gap_for_T_trials = np.array(HG_avg_coverage_gap_for_T_trials)\n","        HG_avg_coverage_gap = np.mean(HG_avg_coverage_gap_for_T_trials)\n","        HG_std_dev_coverage_gap = np.std(HG_avg_coverage_gap_for_T_trials)\n","        \n","        \n","        \n","        # coverage:-\n","        \n","        normal_avg_coverage_for_T_trials = np.array(normal_avg_coverage_for_T_trials)\n","        normal_average_coverage = np.mean(normal_avg_coverage_for_T_trials)\n","        normal_std_dev_coverage = np.std(normal_avg_coverage_for_T_trials, ddof=1)\n","        \n","        LG_avg_coverage_for_T_trials = np.array(LG_avg_coverage_for_T_trials)\n","        LG_average_coverage = np.mean(LG_avg_coverage_for_T_trials)\n","        LG_std_dev_coverage = np.std(LG_avg_coverage_for_T_trials, ddof=1)\n","        \n","        HG_avg_coverage_for_T_trials = np.array(HG_avg_coverage_for_T_trials)\n","        HG_average_coverage = np.mean(HG_avg_coverage_for_T_trials)\n","        HG_std_dev_coverage = np.std(HG_avg_coverage_for_T_trials, ddof=1)\n","        \n","        \n","        return normal_average_set_size_len, normal_std_dev_set_size_len, LG_average_set_size_len, LG_std_dev_set_size_len, HG_average_set_size_len, HG_std_dev_set_size_len, normal_avg_coverage_gap, normal_std_dev_coverage_gap, LG_avg_coverage_gap, LG_std_dev_coverage_gap, HG_avg_coverage_gap, HG_std_dev_coverage_gap, normal_average_coverage, normal_std_dev_coverage, LG_average_coverage, LG_std_dev_coverage, HG_average_coverage, HG_std_dev_coverage\n","    \n","    \n","    \n","    \n","    elif expt_no == 3:\n","        perecentage_of_overlap_for_T_trials = np.array(perecentage_of_overlap_for_T_trials)\n","        average_perecentage_of_overlap_for_T_trials = np.mean(perecentage_of_overlap_for_T_trials)\n","        std_dev_perecentage_of_overlap_for_T_trials = np.std(perecentage_of_overlap_for_T_trials, ddof=1)\n","        \n","        return average_perecentage_of_overlap_for_T_trials, std_dev_perecentage_of_overlap_for_T_trials\n","    \n","    \n","    elif expt_no == 4:\n","        confusion_set_Overlap_metric_for_T_trials = np.array(confusion_set_Overlap_metric_for_T_trials)\n","        average_confusion_set_Overlap_metric_for_T_trials = np.mean(confusion_set_Overlap_metric_for_T_trials)\n","        std_dev_confusion_set_Overlap_metric_for_T_trials = np.std(confusion_set_Overlap_metric_for_T_trials, ddof=1)\n","        \n","        return average_confusion_set_Overlap_metric_for_T_trials, std_dev_confusion_set_Overlap_metric_for_T_trials\n","        \n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Grid search for hyperparameter tuning(k_reg, lambd) for RAPS method"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-08-21T12:25:30.149199Z","iopub.status.busy":"2024-08-21T12:25:30.148637Z","iopub.status.idle":"2024-08-21T12:36:40.057224Z","shell.execute_reply":"2024-08-21T12:36:40.055803Z","shell.execute_reply.started":"2024-08-21T12:25:30.149155Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["k_reg :- 1, lambd :- 0\n","k_reg :- 1, lambd :- 0.0001\n","k_reg :- 1, lambd :- 0.001\n","k_reg :- 1, lambd :- 0.01\n","k_reg :- 1, lambd :- 0.02\n","k_reg :- 1, lambd :- 0.05\n","k_reg :- 1, lambd :- 0.2\n","k_reg :- 1, lambd :- 0.5\n","k_reg :- 1, lambd :- 0.7\n","k_reg :- 1, lambd :- 1.0\n","k_reg :- 2, lambd :- 0\n","k_reg :- 2, lambd :- 0.0001\n","k_reg :- 2, lambd :- 0.001\n","k_reg :- 2, lambd :- 0.01\n","k_reg :- 2, lambd :- 0.02\n","k_reg :- 2, lambd :- 0.05\n","k_reg :- 2, lambd :- 0.2\n","k_reg :- 2, lambd :- 0.5\n","k_reg :- 2, lambd :- 0.7\n","k_reg :- 2, lambd :- 1.0\n","k_reg :- 3, lambd :- 0\n","k_reg :- 3, lambd :- 0.0001\n","k_reg :- 3, lambd :- 0.001\n","k_reg :- 3, lambd :- 0.01\n","k_reg :- 3, lambd :- 0.02\n","k_reg :- 3, lambd :- 0.05\n","k_reg :- 3, lambd :- 0.2\n","k_reg :- 3, lambd :- 0.5\n","k_reg :- 3, lambd :- 0.7\n","k_reg :- 3, lambd :- 1.0\n","k_reg :- 4, lambd :- 0\n","k_reg :- 4, lambd :- 0.0001\n","k_reg :- 4, lambd :- 0.001\n","k_reg :- 4, lambd :- 0.01\n","k_reg :- 4, lambd :- 0.02\n","k_reg :- 4, lambd :- 0.05\n","k_reg :- 4, lambd :- 0.2\n","k_reg :- 4, lambd :- 0.5\n","k_reg :- 4, lambd :- 0.7\n","k_reg :- 4, lambd :- 1.0\n","k_reg :- 5, lambd :- 0\n","k_reg :- 5, lambd :- 0.0001\n","k_reg :- 5, lambd :- 0.001\n","k_reg :- 5, lambd :- 0.01\n","k_reg :- 5, lambd :- 0.02\n","k_reg :- 5, lambd :- 0.05\n","k_reg :- 5, lambd :- 0.2\n","k_reg :- 5, lambd :- 0.5\n","k_reg :- 5, lambd :- 0.7\n","k_reg :- 5, lambd :- 1.0\n","k_reg :- 6, lambd :- 0\n","k_reg :- 6, lambd :- 0.0001\n","k_reg :- 6, lambd :- 0.001\n","k_reg :- 6, lambd :- 0.01\n","k_reg :- 6, lambd :- 0.02\n","k_reg :- 6, lambd :- 0.05\n","k_reg :- 6, lambd :- 0.2\n","k_reg :- 6, lambd :- 0.5\n","k_reg :- 6, lambd :- 0.7\n","k_reg :- 6, lambd :- 1.0\n","k_reg :- 7, lambd :- 0\n","k_reg :- 7, lambd :- 0.0001\n","k_reg :- 7, lambd :- 0.001\n","k_reg :- 7, lambd :- 0.01\n","k_reg :- 7, lambd :- 0.02\n","k_reg :- 7, lambd :- 0.05\n","k_reg :- 7, lambd :- 0.2\n","k_reg :- 7, lambd :- 0.5\n","k_reg :- 7, lambd :- 0.7\n","k_reg :- 7, lambd :- 1.0\n"]}],"source":["\n","expt_no = 4\n","Trials = 10\n","alpha = 0.01\n","k_reg = [1, 2, 3, 4, 5, 6, 7]\n","lambd = [0, 0.0001, 0.001, 0.01, 0.02, 0.05, 0.2, 0.5, 0.7, 1.0]\n","rand = True\n","\n","dic_expt_3 = {\n","    'average_perecentage_of_overlap_for_T_trials' : [],\n","    'std_dev_perecentage_of_overlap_for_T_trials' : []\n","}\n","\n","\n","dic_expt_4 = {\n","    'average_confusion_set_Overlap_metric_for_T_trials' : [],\n","    'std_dev_confusion_set_Overlap_metric_for_T_trials' : []\n","   \n","}\n","\n","\n","\n","dic_expt_1 = {\n","    'k_reg_and_lambd': [],\n","    'average_len' : [],\n","    'std_dev_len' : [],\n","    'average_coverage_gap':[],\n","    'std_dev_coverage_gap' : [],\n","    'average_coverage' : [],\n","    'std_dev_coverage' : []\n"," }\n","\n","\n","\n","\n","dic_expt_2 = {\n","    'k_reg_and_lambd': [],\n","    'normal_average_set_size_len' : [],\n","    'normal_std_dev_set_size_len' : [],\n","    'LG_average_set_size_len' : [],\n","    'LG_std_dev_set_size_len' : [],\n","    'HG_average_set_size_len' : [],\n","    'HG_std_dev_set_size_len' : [],\n","    \n","    'normal_avg_coverage_gap' : [],\n","    'normal_std_dev_coverage_gap' : [],\n","    'LG_avg_coverage_gap' : [],\n","    'LG_std_dev_coverage_gap' : [],\n","    'HG_avg_coverage_gap' : [],\n","    'HG_std_dev_coverage_gap' : [],\n","    \n","    'normal_average_coverage' : [],\n","    'normal_std_dev_coverage' : [],\n","    'LG_average_coverage' : [],\n","    'LG_std_dev_coverage' : [],\n","    'HG_average_coverage' : [],\n","    'HG_std_dev_coverage' : []\n","    \n","}\n","\n","\n","\n","for i in range(len(k_reg)):\n","    for j in range(len(lambd)):\n","        print(f'k_reg :- {k_reg[i]}, lambd :- {lambd[j]}')\n","        #average_len, std_dev_len, average_coverage_gap, std_dev_coverage_gap, average_coverage, std_dev_coverage = main(expt_no, Trials, alpha, k_reg[i], lambd[j], rand)\n","        main(expt_no, Trials, alpha, k_reg[i], lambd[j], rand)\n","        if expt_no == 1:\n","            average_len, std_dev_len, average_coverage_gap, std_dev_coverage_gap, average_coverage, std_dev_coverage = main(expt_no, Trials, alpha, k_reg[i], lambd[j], rand)\n","            dic_expt_1['k_reg_and_lambd'].append((k_reg[i], lambd[j]))\n","            dic_expt_1['average_len'].append(average_len)\n","            dic_expt_1['std_dev_len'].append(std_dev_len)\n","            dic_expt_1['average_coverage_gap'].append(average_coverage_gap)\n","            dic_expt_1['std_dev_coverage_gap'].append(std_dev_coverage_gap)\n","            dic_expt_1['average_coverage'].append(average_coverage)\n","            dic_expt_1['std_dev_coverage'].append(std_dev_coverage)\n","        \n","        elif expt_no == 2:\n","            normal_average_set_size_len, normal_std_dev_set_size_len, LG_average_set_size_len, LG_std_dev_set_size_len, HG_average_set_size_len, HG_std_dev_set_size_len, normal_avg_coverage_gap, normal_std_dev_coverage_gap, LG_avg_coverage_gap, LG_std_dev_coverage_gap, HG_avg_coverage_gap, HG_std_dev_coverage_gap, normal_average_coverage, normal_std_dev_coverage, LG_average_coverage, LG_std_dev_coverage, HG_average_coverage, HG_std_dev_coverage = main(expt_no, Trials, alpha, k_reg[i], lambd[j], rand)\n","            dic_expt_2['k_reg_and_lambd'].append((k_reg[i], lambd[j]))\n","            \n","            dic_expt_2['normal_average_set_size_len'].append(normal_average_set_size_len)\n","            dic_expt_2['normal_std_dev_set_size_len'].append(normal_std_dev_set_size_len)\n","            dic_expt_2['LG_average_set_size_len'].append(LG_average_set_size_len)\n","            dic_expt_2['LG_std_dev_set_size_len'].append(LG_std_dev_set_size_len)\n","            dic_expt_2['HG_average_set_size_len'].append(HG_average_set_size_len)\n","            dic_expt_2['HG_std_dev_set_size_len'].append(HG_std_dev_set_size_len)\n","            \n","            dic_expt_2['normal_avg_coverage_gap'].append(normal_avg_coverage_gap)\n","            dic_expt_2['normal_std_dev_coverage_gap'].append(normal_std_dev_coverage_gap)\n","            dic_expt_2['LG_avg_coverage_gap'].append(LG_avg_coverage_gap)\n","            dic_expt_2['LG_std_dev_coverage_gap'].append(LG_std_dev_coverage_gap)\n","            dic_expt_2['HG_avg_coverage_gap'].append(HG_avg_coverage_gap)\n","            dic_expt_2['HG_std_dev_coverage_gap'].append(HG_std_dev_coverage_gap)\n","            \n","            dic_expt_2['normal_average_coverage'].append(normal_average_coverage)\n","            dic_expt_2['normal_std_dev_coverage'].append(normal_std_dev_coverage)\n","            dic_expt_2['LG_average_coverage'].append(LG_average_coverage)\n","            dic_expt_2['LG_std_dev_coverage'].append(LG_std_dev_coverage)\n","            dic_expt_2['HG_average_coverage'].append(HG_average_coverage)\n","            dic_expt_2['HG_std_dev_coverage'].append(HG_std_dev_coverage)\n","            \n","            \n","            \n","        elif expt_no == 3:\n","            average_perecentage_of_overlap_for_T_trials, std_dev_perecentage_of_overlap_for_T_trials = main(expt_no, Trials, alpha, k_reg[i], lambd[j], rand) \n","            dic_expt_3['average_perecentage_of_overlap_for_T_trials'].append(average_perecentage_of_overlap_for_T_trials)\n","            dic_expt_3['std_dev_perecentage_of_overlap_for_T_trials'].append(std_dev_perecentage_of_overlap_for_T_trials)\n","            \n","            \n","        elif expt_no == 4:\n","            average_confusion_set_Overlap_metric_for_T_trials, std_dev_confusion_set_Overlap_metric_for_T_trials = main(expt_no, Trials, alpha, k_reg[i], lambd[j], rand)\n","            dic_expt_4['average_confusion_set_Overlap_metric_for_T_trials'].append(average_confusion_set_Overlap_metric_for_T_trials)\n","            dic_expt_4['std_dev_confusion_set_Overlap_metric_for_T_trials'].append(std_dev_confusion_set_Overlap_metric_for_T_trials)\n","            \n","            "]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-08-21T12:45:55.138181Z","iopub.status.busy":"2024-08-21T12:45:55.137749Z","iopub.status.idle":"2024-08-21T12:45:55.153240Z","shell.execute_reply":"2024-08-21T12:45:55.151945Z","shell.execute_reply.started":"2024-08-21T12:45:55.138147Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>average_confusion_set_Overlap_metric_for_T_trials</th>\n","      <th>std_dev_confusion_set_Overlap_metric_for_T_trials</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>33.418803</td>\n","      <td>1.930830</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>29.218559</td>\n","      <td>2.346914</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>25.170940</td>\n","      <td>1.225757</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>22.905983</td>\n","      <td>2.019231</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>21.587302</td>\n","      <td>1.841244</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>34.065934</td>\n","      <td>2.211889</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>32.515263</td>\n","      <td>2.896316</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>30.384615</td>\n","      <td>4.433321</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>30.873016</td>\n","      <td>4.012005</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>32.264957</td>\n","      <td>4.320080</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>70 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["    average_confusion_set_Overlap_metric_for_T_trials  \\\n","0                                           33.418803   \n","1                                           29.218559   \n","2                                           25.170940   \n","3                                           22.905983   \n","4                                           21.587302   \n","..                                                ...   \n","65                                          34.065934   \n","66                                          32.515263   \n","67                                          30.384615   \n","68                                          30.873016   \n","69                                          32.264957   \n","\n","    std_dev_confusion_set_Overlap_metric_for_T_trials  \n","0                                            1.930830  \n","1                                            2.346914  \n","2                                            1.225757  \n","3                                            2.019231  \n","4                                            1.841244  \n","..                                                ...  \n","65                                           2.211889  \n","66                                           2.896316  \n","67                                           4.433321  \n","68                                           4.012005  \n","69                                           4.320080  \n","\n","[70 rows x 2 columns]"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["df_90 = pd.DataFrame(dic_expt_4)\n","df_90"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-08-21T12:45:57.366711Z","iopub.status.busy":"2024-08-21T12:45:57.366299Z","iopub.status.idle":"2024-08-21T12:45:57.375480Z","shell.execute_reply":"2024-08-21T12:45:57.374350Z","shell.execute_reply.started":"2024-08-21T12:45:57.366677Z"},"trusted":true},"outputs":[{"data":{"text/plain":["average_confusion_set_Overlap_metric_for_T_trials    20.873016\n","std_dev_confusion_set_Overlap_metric_for_T_trials     1.839050\n","Name: 5, dtype: float64"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["min_index = df_90['average_confusion_set_Overlap_metric_for_T_trials'].idxmin()\n","min_row = df_90.loc[min_index]\n","min_row"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":9436504,"datasetId":5524218,"sourceId":9247847,"sourceType":"datasetVersion"},{"databundleVersionId":9432843,"datasetId":5524198,"sourceId":9244485,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
